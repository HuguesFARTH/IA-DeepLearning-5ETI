{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff07686",
   "metadata": {},
   "source": [
    "# Loading Data and Data Processing\n",
    "\n",
    "There are different types of data that need to be loaded and processed.\n",
    "1. Texts\n",
    "  * CSV files\n",
    "  * Human readable textual files\n",
    "2. Images\n",
    "3. Audio\n",
    "\n",
    "Data may come from different sources:\n",
    "1. Locally available data\n",
    "2. URL of dataset\n",
    "3. Available datasets from Tensorflow and Kaggle\n",
    "4. Numpy and Pandas arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cf55e",
   "metadata": {},
   "source": [
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fa00d",
   "metadata": {},
   "source": [
    "### CSV files\n",
    "\n",
    "#### Loading CSV files\n",
    "\n",
    "In the following exercise, we see how data is obtained from a CSV file whose URL is known to us and loaded to Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e186b9b",
   "metadata": {},
   "source": [
    "In the first approach, we will directly load the data from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b48910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importing the pandas library\n",
    "\n",
    "csv_data = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aacd69",
   "metadata": {},
   "source": [
    "Print the values of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3e1a9",
   "metadata": {},
   "source": [
    "In the second approach, we will download the data to our disk and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5065ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils # We use utils for downloading the data\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "\n",
    "dataset_dir = utils.get_file(origin=url, cache_dir=\"./\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3ac7",
   "metadata": {},
   "source": [
    "Verify whether the file is downloaded. If the above download is complete, the data will be downloaded to the **datasets** directory in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a714ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = list(listdir(\"./datasets\"))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\n",
    "    \"./datasets/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99708337",
   "metadata": {},
   "source": [
    "#### Preprocessing CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5720be",
   "metadata": {},
   "source": [
    "Suppose that our goal is to predict the age from the other features. \n",
    "\n",
    "We will first see the unique values of Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_data[\"Age\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9056fa",
   "metadata": {},
   "source": [
    "Now we will create separate dataframe for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = csv_data.copy()\n",
    "labels = features.pop('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd72565",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2e2fc",
   "metadata": {},
   "source": [
    "We see only numerical values in the features. We will now create a numpy array of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_features = np.array(features)\n",
    "\n",
    "print(np_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970750af",
   "metadata": {},
   "source": [
    "**Note** It is a good practice to normalize the data before training a tensorflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313968ca",
   "metadata": {},
   "source": [
    "We will use `Normalization` from `tensorflow` for this purpose. \n",
    "Note that the function `adopt` should be run only on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "normalize = layers.Normalization()\n",
    "\n",
    "\n",
    "normalize.adapt(np_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bf8d0",
   "metadata": {},
   "source": [
    "#### Using the data for training\n",
    "\n",
    "Now we are ready to progress with the training.\n",
    "We have the features and the labels ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building a model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  normalize, ## Note the use of Normalization layer\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.optimizers.Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ba959",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting the training\n",
    "\n",
    "model.fit(np_features, labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d51af6",
   "metadata": {},
   "source": [
    "### Text files\n",
    "\n",
    "Now we will focus on the text files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f2858",
   "metadata": {},
   "source": [
    "We will use Stack Overflow dataset for this part.Note that this is a zipped .tar dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f409f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download, unzip and untar the dataset\n",
    "\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "\n",
    "dataset_dir = utils.get_file(\n",
    "    origin=url,\n",
    "    untar=True,\n",
    "    cache_dir='./datasets',\n",
    "    cache_subdir='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a89071",
   "metadata": {},
   "source": [
    "We will now see the contents of the downloaded file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0280da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = list(listdir(\"./datasets\"))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2592f",
   "metadata": {},
   "source": [
    "If the download is correctly done, we can see the `train` and `test` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973eb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(listdir(\"./datasets/train\"))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d044e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(listdir(\"./datasets/train/javascript\"))\n",
    "print(files[:5]) # print the name of first five files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(listdir(\"./datasets/test\"))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970e712",
   "metadata": {},
   "source": [
    "As we can see there are only two folders: `train` and `test`. It is a good practice to create datasets for training, validation and testing.\n",
    "\n",
    "In the following code, we load the training data from the `train` folder and use 20% of the data for validation.\n",
    "\n",
    "So far, we have loaded the complete data for training. We will now create batches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_size_percentage = 0.2 # To ensure 80:20 training:validation \n",
    "seed = 40 # To ensure proper shuffiling\n",
    "\n",
    "train_batch = utils.text_dataset_from_directory(\n",
    "    \"./datasets/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split= validation_size_percentage,\n",
    "    subset='training',\n",
    "    seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f2598",
   "metadata": {},
   "source": [
    "We will now see the details of data: text and the associated label using the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_batch.take(1): # Taking into consideration the first batch\n",
    "    for i in range(batch_size):\n",
    "        print(f'Label: {label_batch[i].numpy()} for the text: {text_batch[i].numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22fb22",
   "metadata": {},
   "source": [
    "But, you may have observed that the labels are not \n",
    "'csharp', 'python', 'java', 'javascript', but rather some numbers 0,1, 2, 3.\n",
    "\n",
    "Now, we will display the associated class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ad86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(train_batch.class_names):\n",
    "  print(\"Label\", i, \"corresponds to\", label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593531c6",
   "metadata": {},
   "source": [
    "Now, we will create the validation dataset. \n",
    "\n",
    "Recall that we used the value 'training' for the parameter subset. This time we will use the value 'validation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feefbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = utils.text_dataset_from_directory(\n",
    "    \"./datasets/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split= validation_size_percentage,\n",
    "    subset='validation',\n",
    "    seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203979e",
   "metadata": {},
   "source": [
    "And finally, we will create the test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69098b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = utils.text_dataset_from_directory(\n",
    "    \"./datasets/test\",\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbabce2",
   "metadata": {},
   "source": [
    "However, these datasets cannot be yet used for training, since we need vectors for working with Tensorflow. \n",
    "\n",
    "Our next goal is to convert the text data to associated vectors.\n",
    "\n",
    "We will use two approaches:\n",
    "1. Binary Vectorization (one-hot encoding)\n",
    "2. 'int' Vectorization (integer indices for each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5c3e4",
   "metadata": {},
   "source": [
    "We will first start with **binary Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocabulary_size = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c719249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_batch.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b863fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_batch.take(1):\n",
    "    for i in range(1):\n",
    "        print(\"Text\", text_batch[0].numpy())\n",
    "        print(\"Label\", label_batch[0].numpy())\n",
    "        print(\"Binary Vectorization:\")\n",
    "        print(binary_vectorize_layer(text_batch[0])) # One-hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee794506",
   "metadata": {},
   "source": [
    "The above output corresponds to one-hot encoding value of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4707e37",
   "metadata": {},
   "source": [
    "We will first start with **'int' Vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9a9ef",
   "metadata": {},
   "source": [
    "Unlike 'Binary' Vectorization, we also need the maximum sequence length.\n",
    "\n",
    "Long texts will be truncated to this maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d18ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocabulary_size = 10000\n",
    "max_sequence_length = 200\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_sequence_length=max_sequence_length,\n",
    "    output_mode='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_batch.map(lambda text, labels: text)\n",
    "int_vectorize_layer.adapt(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_batch.take(1):\n",
    "    for i in range(1):\n",
    "        print(\"Text\", text_batch[0].numpy())\n",
    "        print(\"Label\", label_batch[0].numpy())\n",
    "        print(\"Binary Vectorization:\")\n",
    "        print(int_vectorize_layer(text_batch[0])) # Check the length of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107524c9",
   "metadata": {},
   "source": [
    "To understand what the above sequence signify, we will look up in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d62005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in int_vectorize_layer(text_batch[0]):\n",
    "    print(f\"{i}: \", int_vectorize_layer.get_vocabulary()[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c872c",
   "metadata": {},
   "source": [
    "### References\n",
    "1. [Tensorflow: Load CSV data](https://www.tensorflow.org/tutorials/load_data/csv?hl=en)\n",
    "2. [Tensorflow: Load text](https://www.tensorflow.org/tutorials/load_data/text?hl=en])\n",
    "3. [tf.keras.utils.text_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory)\n",
    "4. [tf.keras.utils.get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)\n",
    "5. [tf.keras.layers.TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
